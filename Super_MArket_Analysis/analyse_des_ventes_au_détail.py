# -*- coding: utf-8 -*-
"""Analyse des ventes au dÃ©tail.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uvX0v7lIApqHk8gDXd0n2ewqHRMldAaT
"""

# pip install pyspark

"""# **Analyse des ventes au dÃ©tail**

# **LA DATA ECLAIRE LES DYNAMIQUES DE VENTES ET GUIDE LES DECISIONS**
"""



"""

### DÃ©finitions du Dataset "Supermarket Sales"

Ce tableau de donnÃ©es contient les informations clÃ©s pour analyser les ventes d'un supermarchÃ©. Chaque colonne est essentielle pour comprendre le comportement des clients, les performances des produits et les tendances de vente.

---

### **DÃ©tails de la transaction**

* **`Invoice ID`** : L'identifiant unique de chaque facture, permettant de distinguer chaque transaction.
* **`Date`**  & **`Time`**  : La date et l'heure exactes de la transaction.
* **`Payment`** : La mÃ©thode de paiement utilisÃ©e (espÃ¨ces, carte de crÃ©dit ou porte-monnaie Ã©lectronique).
* **`Rating`**  : La note de satisfaction donnÃ©e par le client, sur une Ã©chelle de 1 Ã  10.

---

### **Produits & Finances**

* **`Product line`**  : La catÃ©gorie du produit (par exemple, "santÃ© et beautÃ©" ou "accessoires Ã©lectroniques").
* **`Unit price`** : Le prix d'une seule unitÃ© du produit.
* **`Quantity`**  : Le nombre d'unitÃ©s achetÃ©es dans la transaction.
* **`cogs`** : Le **coÃ»t des marchandises vendues**, c'est-Ã -dire le montant total de la vente hors taxes.
* **`Tax 5%`**  : La TVA de 5 % appliquÃ©e sur le montant total des produits.
* **`Sales`** : Le montant total de la vente, incluant la TVA.
* **`gross margin percentage`** : Le pourcentage de la marge brute, qui est fixe Ã  environ 4,76 % pour ce dataset.
* **`gross income`** : Le bÃ©nÃ©fice brut, qui correspond au montant de la TVA payÃ©e.

---

### **Informations sur le client & le magasin**

* **`City`** : La ville oÃ¹ la transaction a eu lieu : Yangon, Naypyitaw ou Mandalay.
* **`Branch`**  : Le code de la succursale du supermarchÃ© (A, B ou C).
* **`Customer type`**  : Le statut du client â€” **membre** du programme de fidÃ©litÃ© ou client **occasionnel**.
* **`Gender`**  : Le genre du client."""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

from pyspark.sql.functions import col , sum
from pyspark.sql import functions as F

import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

spark = SparkSession.builder.appName('analyse_vente').getOrCreate()



"""# **Chargement du DataSet**"""

# !ls "/content/drive/MyDrive"

data = spark.read.format("csv").options(header=True, inferSchema=True).load("/content/drive/MyDrive/SuperMarket_Analysis.csv")

"""# **Exploration des donnÃ©es**"""

data.toPandas()



print(f" {data.count()} ligne(s) et {len(data.columns)} colonne(s)")



data.printSchema()

data.describe().show()



from pyspark.sql.types import NumericType

colonnes_numeriques = [ colonne.name for colonne in data.schema.fields if isinstance(colonne.dataType , NumericType) ]
print(f" les colonnes numeriques : {colonnes_numeriques}")

for colonne in colonnes_numeriques :
  data.select(colonne).describe().show()



import seaborn as sns
import matplotlib.pyplot as plt
from pyspark.sql.types import NumericType

colonnes_numeriques = [colonne.name for colonne in data.schema.fields if isinstance(colonne.dataType, NumericType)]
print(f"Les colonnes numÃ©riques : {colonnes_numeriques}")



data_pd = data.select(colonnes_numeriques).toPandas()

for col in colonnes_numeriques:
    plt.figure(figsize=(8,4))
    sns.displot(data_pd[col], kde=True, bins=30)
    plt.title(f"Distribution de {col}")
    plt.xlabel(col)
    plt.ylabel("FrÃ©quence")
    plt.show()



data.select("City").distinct().show()

data.groupBy("Product line").count().show()

data.groupBy("Gender").count().show()

"""*Le DataFrame est composÃ© de plus de femme avec 571 et 429 hommes*"""

data.select("Branch").distinct().show()

"""# **Nettoyage & prÃ©paration**"""

for col in data.columns:
  print(col , ":" , data.filter( data[col] == "?").count() )

[ (name,dtype) for name , dtype in data.dtypes]

# for column in data.columns :
#   a = data.filter( col(column).isNull() ).count()
#   print(f"{column} : {a}")



from pyspark.sql.functions import to_date, col

data = data.withColumn("Date", to_date(col("Date"), "M/d/yyyy"))





"""# **Vue dâ€™ensemble**

**Chiffre dâ€™affaires global**
"""

Ca_total = data.agg(F.sum('Sales').alias('CA_TOTAAL'))
Ca_total.show()

"""**Marge totale**"""

marge_totale = data.agg(F.sum('gross income').alias('MARGE_TOTALE'))
marge_totale.show()

"""**Panier moyen**"""

panier_moyen = data.groupBy('Invoice ID').agg(F.sum('Sales').alias('venteparfacture')).agg(F.avg('venteparfacture').alias('PANIER_MOYEN'))
panier_moyen.show()



from pyspark.sql.types import NumericType

colonnes_numeriques = [colonne.name for colonne in data.schema.fields if isinstance(colonne.dataType , NumericType)]
print(f"Les colonnes numÃ©riques : {colonnes_numeriques}")

# Conversion
data_pd = data.select(colonnes_numeriques).toPandas()

corr_matrix = data_pd.corr()
print(corr_matrix)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", center=0)
plt.title("Correlation ", fontsize=16)
plt.show()





"""# **Analyse gÃ©ographique**

**Chiffre d'affaires par ville**
"""

Ca_ville = data.groupBy('City').agg(F.sum('Sales').alias('CA/VILLE')).orderBy(F.desc('CA/VILLE'))
Ca_ville.show()

ca_par_ville = Ca_ville.toPandas()
plt.bar(ca_par_ville['City'] , ca_par_ville['CA/VILLE']  )
plt.show()



"""**CA & MARGE / VILLE**"""

VCAM = data.groupBy('City').agg(F.sum('Sales').alias('CA') , F.sum('gross income').alias('MARGE'))
VCAM.show()



"""**Les produits les plus consommÃ©s / ville**"""

from pyspark.sql import functions as F

qte_produit_ville = (data.groupBy("City", "Product line").agg(F.sum("Quantity").alias("Total_Quantity")))
qte_produit_ville.show()

from pyspark.sql.window import Window

windowSpec = Window.partitionBy("City").orderBy(F.desc("Total_Quantity"))


produit_top_ville = (qte_produit_ville.withColumn("rank", F.row_number().over(windowSpec)).filter(F.col("rank") == 1).drop("rank"))

produit_top_ville.show()





"""**NBR TRANSACTION / VILLE**"""

TV = data.groupBy('City').agg(F.count_distinct('Invoice ID').alias('TRANSACTION/VILLE'))
TV.show()



from pyspark.sql.functions import sum, count, col

# ======================= Calcul du chiffre d'affaires total et du nombre de transactions par ville =======================
panier_moyen_df = data.groupBy("City").agg(sum("Sales").alias("total_sales"),count("*").alias("number_of_transactions"))



# ======================= Calcul du panier moyen en divisant le chiffre d'affaires total par le nombre de transactions =======================
panier_moyen_df = panier_moyen_df.withColumn("Panier Moyen",col("total_sales") / col("number_of_transactions"))


panier_moyen_df.show()



"""**Chiffre d'affaires par ville en fonction du mois**"""

ventes_janvier = data.filter((data["Date"] >= "2019-01-01") & (data["Date"] <= "2019-01-31"))
ventes_janvier.groupBy("City").agg(F.sum("Sales").alias("CA_Janvier")).orderBy("CA_Janvier").show()



vente_fevrier = data.filter( (data["Date"] >= "2019-02-01" ) & (data["Date"] <= "2019-02-28" ) )
vente_fevrier.groupBy("City").agg(F.sum("Sales").alias("CA_Fevrier")).orderBy("CA_Fevrier").show()



vente_fevrier = data.filter( (data["Date"] >= "2019-03-01" ) & (data["Date"] <= "2019-03-31" ) )
vente_fevrier.groupBy("City").agg(F.sum("Sales").alias("CA_Mars")).orderBy("CA_Mars").show()



"""**Moyens de paiement /VILLE**"""

payment_count_by_city = data.groupBy('City', 'Payment').count().orderBy('City', 'count', ascending=False)
payment_count_by_city.show()



satisfaction = data.groupBy('City').agg(F.avg('Rating').alias('satisfaction_moyenne')).orderBy(F.desc('satisfaction_moyenne'))
satisfaction.show()

SV = satisfaction.toPandas()

plt.figure(figsize=(8, 6))
plt.pie(SV['satisfaction_moyenne'] ,  labels = SV['City']  , autopct='%1.1F%%' )
plt.title("satisfaction moyenne par ville")
plt.axis('equal')
plt.show()



"""# **PRODUIT**"""

produit_prix = data.groupBy("Product line").agg(F.max("Unit price").alias("Max_Unit_Price")).orderBy(F.desc("Max_Unit_Price"))
produit_prix.show()

"""**CA / PRODUIT**"""

Ca_produit = data.groupBy('Product line').agg(F.sum('Sales').alias('CA_PRODUIT')).orderBy(F.desc('CA_PRODUIT'))
Ca_produit.show()

ca_produit = Ca_produit.toPandas()

plt.figure(figsize=(8,6))
plt.bar( ca_produit['Product line'] , ca_produit['CA_PRODUIT'] )
plt.xticks(rotation=45)
plt.show()



"""**CA & QT / PRODUIT**"""

top_products = data.groupBy("Product line").agg(sum("Quantity").alias("Total_Quantity"), sum("Sales").alias("Total_Sales")).orderBy(desc("Total_Sales"))
top_products.show()



satisfaction_P = data.groupBy('Product line').agg(F.avg('Rating').alias('satisfaction_moyenne')).orderBy(F.desc('satisfaction_moyenne'))
satisfaction_P.show()

SP = satisfaction_P.toPandas()

plt.figure(figsize=(10, 8))
plt.pie(SP['satisfaction_moyenne'] ,  labels = SP['Product line']  , autopct='%1.1F%%' )
plt.title("satisfaction moyenne par produit")
plt.axis('equal')
plt.show()





from pyspark.sql.functions import month, year , col

data = data.withColumn("Year", year(col("Date")))
data = data.withColumn("Month", month("Date"))
data = data.withColumn("Year", year("Date"))

ventes_mensuelles = data.groupBy("Year", "Month").agg(F.sum("Sales").alias("CA_Mensuel")).orderBy("Year", "Month")

ventes_mensuelles.show()

ventes_mensuelles = data.groupBy("Month").agg(F.sum("Sales").alias("CA_Mensuel")).orderBy("Month")

ventes_M = ventes_mensuelles.toPandas()

plt.figure(figsize=(10,5))
plt.plot(ventes_M["Month"], ventes_M["CA_Mensuel"], marker="o")
plt.title("Ã‰volution du CA mensuel sur l'annÃ©e")
plt.xlabel("Mois")
plt.ylabel("Chiffre dâ€™affaires")
plt.xticks(range(1,13))
plt.grid(True)
plt.show()





data.select("Year").distinct().orderBy("Year").show()

data.select("Month").distinct().orderBy("Month").show()

data.select("Year", "Month").distinct().orderBy("Year", "Month").show()



ventes_par_date = data.groupBy("Date").agg(sum("Sales").alias("CA_journalier")).orderBy("Date")
ventes_par_date.show()

pdf = ventes_par_date.toPandas()
plt.figure(figsize=(12,6))
plt.plot(pdf["Date"], pdf["CA_journalier"])
plt.title("Ã‰volution du CA par date")
plt.xlabel("Date")
plt.ylabel("Chiffre dâ€™affaires")
plt.xticks(rotation=45)
plt.show()



from pyspark.sql.functions import dayofweek, when, col

# ================ dayofweek() : 1 = dimanche, 2 = lundi, ..., 7 = samedi ===================
data = data.withColumn("day_of_week", dayofweek(col("Date")))



# ======================= la colonne WeekType ========================
data = data.withColumn(
    "WeekType",
    when(col("day_of_week").isin(1, 7), "Weekend").otherwise("Semaine")
)

data.select("Date", "day_of_week", "WeekType").show(10)

VSW = data.groupBy("WeekType").agg(F.sum("Sales").alias("CA"))
VSW.show()

vsw = VSW.toPandas()

plt.figure(figsize=(8,6))
plt.pie(vsw['CA'] ,  labels = vsw['WeekType']  , autopct='%1.1F%%' )
plt.title("CA par type de semaine")
plt.show()





"""# **Clients**"""

comparaison_clients = data.groupBy("Customer type").agg(avg("Sales").alias("Moyenne_Sales"), sum("Sales").alias("CA_total")).orderBy(desc("CA_total"))
comparaison_clients.show()

Compa_client = comparaison_clients.toPandas()
plt.bar(Compa_client['Customer type'] , Compa_client['CA_total']  )
plt.title("Chiffre d'affaires par type de client")
plt.xlabel("Type de client")
plt.ylabel("Chiffre d'affaires  (CA)")
plt.show()





"""# **Moyen de paiement**"""

CP = data.groupBy('Payment').agg(F.count('Sales').alias('NBR/PAIEMENT'))
CP.show()

nbr_v_par_paiement = CP.toPandas()
plt.pie(nbr_v_par_paiement['NBR/PAIEMENT'] , labels = nbr_v_par_paiement['Payment'] , autopct='%1.1F%%' )
plt.title("RÃ©partition des modes de paiement ")
plt.show()







"""# **Conclusion de lâ€™analyse des ventes**

Parfait ğŸ‘ tu as dÃ©jÃ  une excellente base.
LÃ , tes nouveaux rÃ©sultats apportent encore plus de **profondeur** Ã  ton analyse, donc ta conclusion peut Ãªtre enrichie pour montrer :

1. **La rÃ©partition gÃ©ographique du CA et des marges**
2. **Les diffÃ©rences de comportement par ville** (transactions, panier moyen, produits favoris)
3. **La perception des prix et lâ€™Ã©ventail des produits**

---

Voici une version reformulÃ©e et enrichie de ta conclusion en Markdown pour ton notebook :

---

# ğŸ“Š SynthÃ¨se des performances commerciales

## âš¡ Performances globales

* **Chiffre d'affaires total** : 322 966
* **Marge totale** : 15 379
* **Panier moyen global** : 322

Ces chiffres confirment une performance commerciale solide, avec une marge nette d'environ **4,8%**.

---

## ğŸ™ï¸ Performance par ville

* **Naypyitaw** :

  * **CA le plus Ã©levÃ©** : 110 568
  * **Marge la plus importante** : 5 265
  * **Nombre de transactions le plus faible** : 328
  * **Panier moyen le plus Ã©levÃ©** : 337

  ğŸ‘‰ Cela montre que **la rentabilitÃ© provient de la valeur des transactions** plutÃ´t que de leur volume.

* **Yangon** :

  * **CA proche de Naypyitaw** : 106 200
  * **Transactions les plus nombreuses** : 340
  * **Panier moyen plus bas** : 312

  ğŸ‘‰ Ici, la stratÃ©gie repose sur **le volume des ventes** et non la valeur par transaction.

* **Mandalay** :

  * **CA similaire Ã  Yangon** : 106 197
  * **Transactions moyennes** : 332
  * **Panier moyen intermÃ©diaire** : 319

  ğŸ‘‰ Position intermÃ©diaire entre Naypyitaw et Yangon.

---

## ğŸ›’ Produits les plus consommÃ©s par ville

* **Mandalay** â†’ *Sports and travel* (322 ventes)
* **Naypyitaw** â†’ *Food and beverages* (369 ventes)
* **Yangon** â†’ *Home and lifestyle* (371 ventes)

Ces diffÃ©rences illustrent des **prÃ©fÃ©rences locales nettes**, utiles pour des campagnes marketing ciblÃ©es.

---

## ğŸ’³ Tendances de paiement et clientÃ¨le

* **Mode de paiement dominant** : l'**E-wallet**, sauf Ã  Naypyitaw oÃ¹ le **cash** reste majoritaire.
* **Type de client le plus rentable** : les **membres fidÃ¨les**, avec **189 694** de CA gÃ©nÃ©rÃ©.

ğŸ‘‰ La fidÃ©lisation client est un vÃ©ritable levier de croissance.

---

## ğŸ’° Structure des prix

Le prix unitaire maximum atteint environ **100** dans toutes les catÃ©gories.
Cela montre une **uniformitÃ© des plafonds tarifaires** entre les lignes de produits, avec des variations faibles dâ€™une catÃ©gorie Ã  lâ€™autre.

---

## ğŸ“… Performances selon les jours

* **Ventes en semaine** : 222 388 (**68% du CA**)
* **Ventes le week-end** : 100 578

ğŸ‘‰ Lâ€™activitÃ© est donc **principalement concentrÃ©e en semaine**, ce qui doit guider les politiques de stocks et de ressources.

---

âœ¨ **En rÃ©sumÃ©** : Naypyitaw est la ville la plus rentable grÃ¢ce Ã  un **panier moyen Ã©levÃ©**, alors que Yangon repose sur un **volume plus important de transactions**.
Les comportements dâ€™achat diffÃ¨rent fortement selon la ville (produits et modes de paiement), ce qui ouvre la voie Ã  des **stratÃ©gies locales personnalisÃ©es**.

---

Veux-tu que je te prÃ©pare aussi une **visualisation finale (dash-style avec barplots/piecharts)** pour accompagner cette conclusion dans ton notebook ? Ã‡a mettrait bien en valeur tes trouvailles.
"""



