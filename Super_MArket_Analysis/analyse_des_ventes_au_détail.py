# -*- coding: utf-8 -*-
"""Analyse des ventes au détail.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uvX0v7lIApqHk8gDXd0n2ewqHRMldAaT
"""

# pip install pyspark

"""# **Analyse des ventes au détail**

# **LA DATA ECLAIRE LES DYNAMIQUES DE VENTES ET GUIDE LES DECISIONS**
"""



"""

### Définitions du Dataset "Supermarket Sales"

Ce tableau de données contient les informations clés pour analyser les ventes d'un supermarché. Chaque colonne est essentielle pour comprendre le comportement des clients, les performances des produits et les tendances de vente.

---

### **Détails de la transaction**

* **`Invoice ID`** : L'identifiant unique de chaque facture, permettant de distinguer chaque transaction.
* **`Date`**  & **`Time`**  : La date et l'heure exactes de la transaction.
* **`Payment`** : La méthode de paiement utilisée (espèces, carte de crédit ou porte-monnaie électronique).
* **`Rating`**  : La note de satisfaction donnée par le client, sur une échelle de 1 à 10.

---

### **Produits & Finances**

* **`Product line`**  : La catégorie du produit (par exemple, "santé et beauté" ou "accessoires électroniques").
* **`Unit price`** : Le prix d'une seule unité du produit.
* **`Quantity`**  : Le nombre d'unités achetées dans la transaction.
* **`cogs`** : Le **coût des marchandises vendues**, c'est-à-dire le montant total de la vente hors taxes.
* **`Tax 5%`**  : La TVA de 5 % appliquée sur le montant total des produits.
* **`Sales`** : Le montant total de la vente, incluant la TVA.
* **`gross margin percentage`** : Le pourcentage de la marge brute, qui est fixe à environ 4,76 % pour ce dataset.
* **`gross income`** : Le bénéfice brut, qui correspond au montant de la TVA payée.

---

### **Informations sur le client & le magasin**

* **`City`** : La ville où la transaction a eu lieu : Yangon, Naypyitaw ou Mandalay.
* **`Branch`**  : Le code de la succursale du supermarché (A, B ou C).
* **`Customer type`**  : Le statut du client — **membre** du programme de fidélité ou client **occasionnel**.
* **`Gender`**  : Le genre du client."""

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

from pyspark.sql.functions import col , sum
from pyspark.sql import functions as F

import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

spark = SparkSession.builder.appName('analyse_vente').getOrCreate()



"""# **Chargement du DataSet**"""

# !ls "/content/drive/MyDrive"

data = spark.read.format("csv").options(header=True, inferSchema=True).load("/content/drive/MyDrive/SuperMarket_Analysis.csv")

"""# **Exploration des données**"""

data.toPandas()



print(f" {data.count()} ligne(s) et {len(data.columns)} colonne(s)")



data.printSchema()

data.describe().show()



from pyspark.sql.types import NumericType

colonnes_numeriques = [ colonne.name for colonne in data.schema.fields if isinstance(colonne.dataType , NumericType) ]
print(f" les colonnes numeriques : {colonnes_numeriques}")

for colonne in colonnes_numeriques :
  data.select(colonne).describe().show()



import seaborn as sns
import matplotlib.pyplot as plt
from pyspark.sql.types import NumericType

colonnes_numeriques = [colonne.name for colonne in data.schema.fields if isinstance(colonne.dataType, NumericType)]
print(f"Les colonnes numériques : {colonnes_numeriques}")



data_pd = data.select(colonnes_numeriques).toPandas()

for col in colonnes_numeriques:
    plt.figure(figsize=(8,4))
    sns.displot(data_pd[col], kde=True, bins=30)
    plt.title(f"Distribution de {col}")
    plt.xlabel(col)
    plt.ylabel("Fréquence")
    plt.show()



data.select("City").distinct().show()

data.groupBy("Product line").count().show()

data.groupBy("Gender").count().show()

"""*Le DataFrame est composé de plus de femme avec 571 et 429 hommes*"""

data.select("Branch").distinct().show()

"""# **Nettoyage & préparation**"""

for col in data.columns:
  print(col , ":" , data.filter( data[col] == "?").count() )

[ (name,dtype) for name , dtype in data.dtypes]

# for column in data.columns :
#   a = data.filter( col(column).isNull() ).count()
#   print(f"{column} : {a}")



from pyspark.sql.functions import to_date, col

data = data.withColumn("Date", to_date(col("Date"), "M/d/yyyy"))





"""# **Vue d’ensemble**

**Chiffre d’affaires global**
"""

Ca_total = data.agg(F.sum('Sales').alias('CA_TOTAAL'))
Ca_total.show()

"""**Marge totale**"""

marge_totale = data.agg(F.sum('gross income').alias('MARGE_TOTALE'))
marge_totale.show()

"""**Panier moyen**"""

panier_moyen = data.groupBy('Invoice ID').agg(F.sum('Sales').alias('venteparfacture')).agg(F.avg('venteparfacture').alias('PANIER_MOYEN'))
panier_moyen.show()



from pyspark.sql.types import NumericType

colonnes_numeriques = [colonne.name for colonne in data.schema.fields if isinstance(colonne.dataType , NumericType)]
print(f"Les colonnes numériques : {colonnes_numeriques}")

# Conversion
data_pd = data.select(colonnes_numeriques).toPandas()

corr_matrix = data_pd.corr()
print(corr_matrix)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", center=0)
plt.title("Correlation ", fontsize=16)
plt.show()





"""# **Analyse géographique**

**Chiffre d'affaires par ville**
"""

Ca_ville = data.groupBy('City').agg(F.sum('Sales').alias('CA/VILLE')).orderBy(F.desc('CA/VILLE'))
Ca_ville.show()

ca_par_ville = Ca_ville.toPandas()
plt.bar(ca_par_ville['City'] , ca_par_ville['CA/VILLE']  )
plt.show()



"""**CA & MARGE / VILLE**"""

VCAM = data.groupBy('City').agg(F.sum('Sales').alias('CA') , F.sum('gross income').alias('MARGE'))
VCAM.show()



"""**Les produits les plus consommés / ville**"""

from pyspark.sql import functions as F

qte_produit_ville = (data.groupBy("City", "Product line").agg(F.sum("Quantity").alias("Total_Quantity")))
qte_produit_ville.show()

from pyspark.sql.window import Window

windowSpec = Window.partitionBy("City").orderBy(F.desc("Total_Quantity"))


produit_top_ville = (qte_produit_ville.withColumn("rank", F.row_number().over(windowSpec)).filter(F.col("rank") == 1).drop("rank"))

produit_top_ville.show()





"""**NBR TRANSACTION / VILLE**"""

TV = data.groupBy('City').agg(F.count_distinct('Invoice ID').alias('TRANSACTION/VILLE'))
TV.show()



from pyspark.sql.functions import sum, count, col

# ======================= Calcul du chiffre d'affaires total et du nombre de transactions par ville =======================
panier_moyen_df = data.groupBy("City").agg(sum("Sales").alias("total_sales"),count("*").alias("number_of_transactions"))



# ======================= Calcul du panier moyen en divisant le chiffre d'affaires total par le nombre de transactions =======================
panier_moyen_df = panier_moyen_df.withColumn("Panier Moyen",col("total_sales") / col("number_of_transactions"))


panier_moyen_df.show()



"""**Chiffre d'affaires par ville en fonction du mois**"""

ventes_janvier = data.filter((data["Date"] >= "2019-01-01") & (data["Date"] <= "2019-01-31"))
ventes_janvier.groupBy("City").agg(F.sum("Sales").alias("CA_Janvier")).orderBy("CA_Janvier").show()



vente_fevrier = data.filter( (data["Date"] >= "2019-02-01" ) & (data["Date"] <= "2019-02-28" ) )
vente_fevrier.groupBy("City").agg(F.sum("Sales").alias("CA_Fevrier")).orderBy("CA_Fevrier").show()



vente_fevrier = data.filter( (data["Date"] >= "2019-03-01" ) & (data["Date"] <= "2019-03-31" ) )
vente_fevrier.groupBy("City").agg(F.sum("Sales").alias("CA_Mars")).orderBy("CA_Mars").show()



"""**Moyens de paiement /VILLE**"""

payment_count_by_city = data.groupBy('City', 'Payment').count().orderBy('City', 'count', ascending=False)
payment_count_by_city.show()



satisfaction = data.groupBy('City').agg(F.avg('Rating').alias('satisfaction_moyenne')).orderBy(F.desc('satisfaction_moyenne'))
satisfaction.show()

SV = satisfaction.toPandas()

plt.figure(figsize=(8, 6))
plt.pie(SV['satisfaction_moyenne'] ,  labels = SV['City']  , autopct='%1.1F%%' )
plt.title("satisfaction moyenne par ville")
plt.axis('equal')
plt.show()



"""# **PRODUIT**"""

produit_prix = data.groupBy("Product line").agg(F.max("Unit price").alias("Max_Unit_Price")).orderBy(F.desc("Max_Unit_Price"))
produit_prix.show()

"""**CA / PRODUIT**"""

Ca_produit = data.groupBy('Product line').agg(F.sum('Sales').alias('CA_PRODUIT')).orderBy(F.desc('CA_PRODUIT'))
Ca_produit.show()

ca_produit = Ca_produit.toPandas()

plt.figure(figsize=(8,6))
plt.bar( ca_produit['Product line'] , ca_produit['CA_PRODUIT'] )
plt.xticks(rotation=45)
plt.show()



"""**CA & QT / PRODUIT**"""

top_products = data.groupBy("Product line").agg(sum("Quantity").alias("Total_Quantity"), sum("Sales").alias("Total_Sales")).orderBy(desc("Total_Sales"))
top_products.show()



satisfaction_P = data.groupBy('Product line').agg(F.avg('Rating').alias('satisfaction_moyenne')).orderBy(F.desc('satisfaction_moyenne'))
satisfaction_P.show()

SP = satisfaction_P.toPandas()

plt.figure(figsize=(10, 8))
plt.pie(SP['satisfaction_moyenne'] ,  labels = SP['Product line']  , autopct='%1.1F%%' )
plt.title("satisfaction moyenne par produit")
plt.axis('equal')
plt.show()





from pyspark.sql.functions import month, year , col

data = data.withColumn("Year", year(col("Date")))
data = data.withColumn("Month", month("Date"))
data = data.withColumn("Year", year("Date"))

ventes_mensuelles = data.groupBy("Year", "Month").agg(F.sum("Sales").alias("CA_Mensuel")).orderBy("Year", "Month")

ventes_mensuelles.show()

ventes_mensuelles = data.groupBy("Month").agg(F.sum("Sales").alias("CA_Mensuel")).orderBy("Month")

ventes_M = ventes_mensuelles.toPandas()

plt.figure(figsize=(10,5))
plt.plot(ventes_M["Month"], ventes_M["CA_Mensuel"], marker="o")
plt.title("Évolution du CA mensuel sur l'année")
plt.xlabel("Mois")
plt.ylabel("Chiffre d’affaires")
plt.xticks(range(1,13))
plt.grid(True)
plt.show()





data.select("Year").distinct().orderBy("Year").show()

data.select("Month").distinct().orderBy("Month").show()

data.select("Year", "Month").distinct().orderBy("Year", "Month").show()



ventes_par_date = data.groupBy("Date").agg(sum("Sales").alias("CA_journalier")).orderBy("Date")
ventes_par_date.show()

pdf = ventes_par_date.toPandas()
plt.figure(figsize=(12,6))
plt.plot(pdf["Date"], pdf["CA_journalier"])
plt.title("Évolution du CA par date")
plt.xlabel("Date")
plt.ylabel("Chiffre d’affaires")
plt.xticks(rotation=45)
plt.show()



from pyspark.sql.functions import dayofweek, when, col

# ================ dayofweek() : 1 = dimanche, 2 = lundi, ..., 7 = samedi ===================
data = data.withColumn("day_of_week", dayofweek(col("Date")))



# ======================= la colonne WeekType ========================
data = data.withColumn(
    "WeekType",
    when(col("day_of_week").isin(1, 7), "Weekend").otherwise("Semaine")
)

data.select("Date", "day_of_week", "WeekType").show(10)

VSW = data.groupBy("WeekType").agg(F.sum("Sales").alias("CA"))
VSW.show()

vsw = VSW.toPandas()

plt.figure(figsize=(8,6))
plt.pie(vsw['CA'] ,  labels = vsw['WeekType']  , autopct='%1.1F%%' )
plt.title("CA par type de semaine")
plt.show()





"""# **Clients**"""

comparaison_clients = data.groupBy("Customer type").agg(avg("Sales").alias("Moyenne_Sales"), sum("Sales").alias("CA_total")).orderBy(desc("CA_total"))
comparaison_clients.show()

Compa_client = comparaison_clients.toPandas()
plt.bar(Compa_client['Customer type'] , Compa_client['CA_total']  )
plt.title("Chiffre d'affaires par type de client")
plt.xlabel("Type de client")
plt.ylabel("Chiffre d'affaires  (CA)")
plt.show()





"""# **Moyen de paiement**"""

CP = data.groupBy('Payment').agg(F.count('Sales').alias('NBR/PAIEMENT'))
CP.show()

nbr_v_par_paiement = CP.toPandas()
plt.pie(nbr_v_par_paiement['NBR/PAIEMENT'] , labels = nbr_v_par_paiement['Payment'] , autopct='%1.1F%%' )
plt.title("Répartition des modes de paiement ")
plt.show()







"""# **Conclusion de l’analyse des ventes**

Parfait 👍 tu as déjà une excellente base.
Là, tes nouveaux résultats apportent encore plus de **profondeur** à ton analyse, donc ta conclusion peut être enrichie pour montrer :

1. **La répartition géographique du CA et des marges**
2. **Les différences de comportement par ville** (transactions, panier moyen, produits favoris)
3. **La perception des prix et l’éventail des produits**

---

Voici une version reformulée et enrichie de ta conclusion en Markdown pour ton notebook :

---

# 📊 Synthèse des performances commerciales

## ⚡ Performances globales

* **Chiffre d'affaires total** : 322 966
* **Marge totale** : 15 379
* **Panier moyen global** : 322

Ces chiffres confirment une performance commerciale solide, avec une marge nette d'environ **4,8%**.

---

## 🏙️ Performance par ville

* **Naypyitaw** :

  * **CA le plus élevé** : 110 568
  * **Marge la plus importante** : 5 265
  * **Nombre de transactions le plus faible** : 328
  * **Panier moyen le plus élevé** : 337

  👉 Cela montre que **la rentabilité provient de la valeur des transactions** plutôt que de leur volume.

* **Yangon** :

  * **CA proche de Naypyitaw** : 106 200
  * **Transactions les plus nombreuses** : 340
  * **Panier moyen plus bas** : 312

  👉 Ici, la stratégie repose sur **le volume des ventes** et non la valeur par transaction.

* **Mandalay** :

  * **CA similaire à Yangon** : 106 197
  * **Transactions moyennes** : 332
  * **Panier moyen intermédiaire** : 319

  👉 Position intermédiaire entre Naypyitaw et Yangon.

---

## 🛒 Produits les plus consommés par ville

* **Mandalay** → *Sports and travel* (322 ventes)
* **Naypyitaw** → *Food and beverages* (369 ventes)
* **Yangon** → *Home and lifestyle* (371 ventes)

Ces différences illustrent des **préférences locales nettes**, utiles pour des campagnes marketing ciblées.

---

## 💳 Tendances de paiement et clientèle

* **Mode de paiement dominant** : l'**E-wallet**, sauf à Naypyitaw où le **cash** reste majoritaire.
* **Type de client le plus rentable** : les **membres fidèles**, avec **189 694** de CA généré.

👉 La fidélisation client est un véritable levier de croissance.

---

## 💰 Structure des prix

Le prix unitaire maximum atteint environ **100** dans toutes les catégories.
Cela montre une **uniformité des plafonds tarifaires** entre les lignes de produits, avec des variations faibles d’une catégorie à l’autre.

---

## 📅 Performances selon les jours

* **Ventes en semaine** : 222 388 (**68% du CA**)
* **Ventes le week-end** : 100 578

👉 L’activité est donc **principalement concentrée en semaine**, ce qui doit guider les politiques de stocks et de ressources.

---

✨ **En résumé** : Naypyitaw est la ville la plus rentable grâce à un **panier moyen élevé**, alors que Yangon repose sur un **volume plus important de transactions**.
Les comportements d’achat diffèrent fortement selon la ville (produits et modes de paiement), ce qui ouvre la voie à des **stratégies locales personnalisées**.

---

Veux-tu que je te prépare aussi une **visualisation finale (dash-style avec barplots/piecharts)** pour accompagner cette conclusion dans ton notebook ? Ça mettrait bien en valeur tes trouvailles.
"""



